{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from xmltodict import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80799f9d",
   "metadata": {},
   "source": [
    "# Working with Unstructured Data (Part II)\n",
    "\n",
    "## Working with text: Automatic Summarization\n",
    "\n",
    "Today, I am going to show you how to work with text. The particular problem I am going to work on is [*summarization*](https://en.wikipedia.org/wiki/Automatic_summarization).\n",
    "\n",
    "For this task, we need texts of moderate size: not too long, or not too short. News articles are perfect for this purpose. I am going to use several sources. \n",
    "\n",
    "* For English texts I am going to use the [Guardian Newspaper](https://www.theguardian.com/international),\n",
    "* For Turkish texts I am going to use [Milliyet](https://www.milliyet.com.tr/)\n",
    "* For French texts I am going to use [Le Monde](https://www.lemonde.fr/)\n",
    "\n",
    "We are going to pull articles on a specific subject using a service called [RSS Feed](https://en.wikipedia.org/wiki/RSS). Each of these newspapers have their own RSS feeds.\n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "\n",
    "### RSS Feeds\n",
    "\n",
    "Let us start with the Guardian: Guardian's RSS feed has a [predictable pattern](https://www.theguardian.com/help/feeds). For example here are some interesting subjects:\n",
    "\n",
    "1. Economy: https://www.theguardian.com/economy/rss\n",
    "2. Technology: https://www.theguardian.com/technology/rss\n",
    "3. Film: https://www.theguardian.com/film/rss\n",
    "4. NBA: https://www.theguardian.com/sport/nba/rss\n",
    "5. Fashion: https://www.theguardian.com/fashion/rss\n",
    "\n",
    "Each RSS feed is an XML file. We are going to parse it and extract the bits we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e813ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77cf17a2",
   "metadata": {},
   "source": [
    "I am going to write a function that retrieves the important part of an RSS feed from Guardian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a644a66f",
   "metadata": {},
   "source": [
    "Now that we can list news articles from a specific subject, let us look at one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c198008",
   "metadata": {},
   "source": [
    "### Text Scraping and Beautiful Soup\n",
    "\n",
    "The page is written in the markup language [HTML](https://en.wikipedia.org/wiki/HTML) which is a specific form of XML even though HTML is older than XML. In order to parse HTML files to extract the bits we are interested in we are going to use a [text scraper](https://en.wikipedia.org/wiki/Data_scraping) called [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f5fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbde59af",
   "metadata": {},
   "source": [
    "In an HTML document, paragraphs are put between '&lt;p&gt;' and '&lt;/p&gt;'. So, we are going to find and extract those bits only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6d94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1375dfa5",
   "metadata": {},
   "source": [
    "This is still HTML. We need to extract the text and join individual paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf42692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "752ff772",
   "metadata": {},
   "source": [
    "Let us convert what we have done into a function so that we can reuse it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bcb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5daed6d",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "OK. Now, we can pull a news article on a specific topic from the Guardian Newspaper. Remember our original goal: we are going to summarize the text using automated methods. For that, we must split the text into its sentences. The operation is called *Sentence Boundary Disambiguation* and the correct way of doing this is via [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) methods. But today we are going to keep things simple and use [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to split the text. Most sentences end with a '.', '?' or a '!'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84948245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a243e39",
   "metadata": {},
   "source": [
    "Of course, this doesn't work all the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bdc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3bf9355",
   "metadata": {},
   "source": [
    "But, for today's lecture regular expression we used above should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30437a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "675e19e3",
   "metadata": {},
   "source": [
    "While at it, let us clean the text as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e8e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad452ba",
   "metadata": {},
   "source": [
    "## Vectorizing a text\n",
    "\n",
    "A text is a sequence of words that are presented within syntactical units. In our case these units are sentences. But for larger texts, these units can be paragraphs or even chapters. Now, our text contains a large number of words given in a specific order. But for the purpose of this exercise, let us forget the order they are presented. Let us treat each sentence as a bag/multi-set of words. We can convert each sentence to a vector as follows:\n",
    "\n",
    "1. Put all distinct words that appear in our text into an ordered list (no repetitions.)\n",
    "2. Let W be the number of distinct words in our text and let S be the number of sentences in our text.\n",
    "3. Construct an array A of size S x W where rows are marked by sentences while columns are marked by words.\n",
    "4. For each sentence S and word W, set the entry A(S,W) as the number of times the word W appears in the sentence S.\n",
    "\n",
    "The scikit-learn library has a specific function for this task called [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef7f96d",
   "metadata": {},
   "source": [
    "The example text we are using has 44 sentences and 481 unique words.\n",
    "\n",
    "OK. We vectorized the text. Now, what?\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "In the last lecture I used [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) to project large dimensional data onto $\\mathbb{R}^2$ so that we can visualize it. We can also use PCA for summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b3a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9180ae3c",
   "metadata": {},
   "source": [
    "The first number in each item is the weight of the sentence, the second is the position of the sentence in the text and the third is the cleaned version of the sentence. We need to sort this list with respect to weights and take few for the summary. Below, I'll take the 4 sentences with highest weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1de960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f953c93",
   "metadata": {},
   "source": [
    "The result is in the wrong sentence order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25f925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27cec04f",
   "metadata": {},
   "source": [
    "Let us write this as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7f318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b103e38b",
   "metadata": {},
   "source": [
    "## Let us repeat this in Turkish\n",
    "\n",
    "OK. We worked with a text in English. But, observe that what we have done is not specific to a language. We can get summaries using the same method.\n",
    "\n",
    "For this part I am going to use [Milliyet's RSS Feeds](https://www.milliyet.com.tr/milliyet.aspx?atype=rss). They also follow a predictable pattern:\n",
    "\n",
    "* World: https://www.milliyet.com.tr/rss/rssNew/dunyaRss.xml\n",
    "* Economy: https://www.milliyet.com.tr/rss/rssNew/ekonomiRss.xml\n",
    "* Technology: https://www.milliyet.com.tr/rss/rssNew/teknolojiRss.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a4e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7d5227",
   "metadata": {},
   "source": [
    "## Now, in French\n",
    "\n",
    "For this part, we are going to use [France Soir](https://www.francesoir.fr/). Their RSS pattern is also predictable:\n",
    "\n",
    "* Politics: https://www.francesoir.fr/rss-politique.xml\n",
    "* Culture: https://www.francesoir.fr/rss-culture.xml\n",
    "* Opinions: https://www.francesoir.fr/rss-opinions.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944edaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c9e1a0",
   "metadata": {},
   "source": [
    "## What else can we do?\n",
    "\n",
    "We summarized the text by assigning suitable weights to the sentences. But we could do the same with words of the text to figure out the *keywords* within the text. For that we must transpose our count matrix and apply the same PCA method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd9a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
